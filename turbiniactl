#!/usr/bin/env python
#
# Copyright 2017 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Command line interface for Turbinia."""
# pylint: disable=bad-indentation

from __future__ import unicode_literals

import argparse
from datetime import datetime
from datetime import timedelta
import json
import logging
import sys

try:
  import psq
except ImportError:
  print 'PSQ Module cannot be loaded, please see:'
  print 'https://github.com/GoogleCloudPlatform/psq'
  sys.exit(1)

from turbinia import config
from turbinia.config import logger
from turbinia.lib.google_cloud import GoogleCloudFunction
from turbinia import evidence
from turbinia import task_manager
from turbinia import VERSION
from turbinia.pubsub import TurbiniaRequest

log = logging.getLogger('turbinia')
logger.setup()


def PrintTaskStatus(instance, project, region, days=0, task_id=None,
                    request_id=None, all_fields=False):
  """Prints the recent history for Turbinia Tasks.

  Args:
    instance (string): The Turbinia instance name (by default the same as the
        PUBSUB_TOPIC in the config).
    project (string): The name of the project.
    region (string): The name of the zone to execute in.
    days (int): The number of days we want history for.
    task_id (string): The Id of the task.
    request_id (string): The Id of the request we want tasks for.
  """
  function = GoogleCloudFunction(project_id=project, region=region)
  func_args = {'instance': instance, 'kind': 'TurbiniaTask'}

  if days:
    start_time = datetime.now() - timedelta(days=days)
    # Format this like '1990-01-01T00:00:00z' so we can cast it directly to a
    # javascript Date() object in the cloud function.
    start_string = start_time.strftime('%Y-%m-%dT%H:%M:%S')
    func_args.update({'start_time': start_string})
  elif task_id:
    func_args.update({'task_id': task_id})
  elif request_id:
    func_args.update({'request_id': request_id})

  response = function.ExecuteFunction('gettasks', func_args)
  if not response.has_key('result'):
    log.error('No results found')
    print '\nNo results found.\n'
    if response.get('error', '{}') != '{}':
      msg = 'Error executing Cloud Function: [{0!s}].'.format(
          response.get('error'))
      print '{0:s}\n'.format(msg)
      log.error(msg)
    log.debug('GCF response: {0!s}'.format(response))
    sys.exit(1)

  try:
    results = json.loads(response['result'])
  except ValueError as e:
    log.error('Could not deserialize result from GCF: [{0!s}]'.format(e))
    sys.exit(1)

  task_results = results[0]
  num_results = len(task_results)
  if not num_results:
    print '\nNo Tasks found.'
    return

  print '\nRetrieved {0:d} Task results:'.format(num_results)
  for task in task_results:
    if task.get('successful', None):
      success = 'Successful'
    elif task.get('successful', None) is None:
      success = 'Running'
    else:
      success = 'Failed'

    status = task.get('status') if task.get('status') else 'Task in Progress'
    if all_fields:
      print '{0:s} request: {1:s} task: {2:s} {3:s} {4:s}: {5:s}'.format(
          task['last_update'], task['request_id'], task['id'], task['name'],
          success, status)
    else:
      print '{0:s} {1:s} {2:s}: {3:s}'.format(
          task['last_update'], task['name'], success, status)


if __name__ == '__main__':
  # TODO(aarontp): Allow for single run mode when specifying evidence
  #                which will also terminate the task manager after evidence has
  #                been processed.
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '-q', '--quiet', action='store_true', help='Show minimal output')
  parser.add_argument(
      '-v', '--verbose', action='store_true', help='Show verbose output')
  # TODO(aarontp): Turn off debug by default later
  parser.add_argument(
      '-d', '--debug', action='store_true', help='Show debug output',
      default=True)
  parser.add_argument('-o', '--output_dir', help='Directory path for output')
  parser.add_argument('-L', '--log_file', help='Log file')
  parser.add_argument(
      '-S',
      '--server',
      action='store_true',
      help='Run Turbinia Server indefinitely')
  parser.add_argument(
      '-V',
      '--version',
      action='version',
      version=VERSION,
      help='Show the version')
  parser.add_argument(
      '-D',
      '--dump_json',
      action='store_true',
      help='Dump JSON output of Turbinia Request instead of sending it')

  subparsers = parser.add_subparsers(
      dest='command', title='Commands', metavar='<command>')

  # TODO(aarontp): Find better way to specify these that allows for multiple
  # pieces of evidence to be submitted. Maybe automagically create different
  # commands based on introspection of evidence objects?
  # RawDisk
  parser_rawdisk = subparsers.add_parser(
      'rawdisk', help='Process RawDisk as Evidence')
  parser_rawdisk.add_argument(
      '-l', '--local_path', help='Local path to the evidence', required=True)
  parser_rawdisk.add_argument(
      '-s',
      '--source',
      help='Description of the source of the evidence',
      required=False)
  parser_rawdisk.add_argument(
      '-n', '--name', help='Descriptive name of the evidence', required=False)

  # Parser options for Google Cloud Disk Evidence type
  parser_googleclouddisk = subparsers.add_parser(
      'googleclouddisk',
      help='Process Google Cloud Persistent Disk as Evidence')
  parser_googleclouddisk.add_argument(
      '-d', '--disk_name', help='Google Cloud name for disk', required=True)
  parser_googleclouddisk.add_argument(
      '-p', '--project', help='Project that the disk is associated with',
      required=True)
  parser_googleclouddisk.add_argument(
      '-z', '--zone', help='Geographic zone the disk exists in',
      required=True)
  parser_googleclouddisk.add_argument(
      '-s',
      '--source',
      help='Description of the source of the evidence',
      required=False)
  parser_googleclouddisk.add_argument(
      '-n', '--name', help='Descriptive name of the evidence', required=False)

  # Parser options for Google Cloud Persistent Disk Embedded Raw Image
  parser_googleclouddiskembedded = subparsers.add_parser(
      'googleclouddiskembedded',
      help='Process Google Cloud Persistent Disk with an embedded raw disk '
           'image as Evidence')
  parser_googleclouddiskembedded.add_argument(
      '-e', '--embedded_path',
      help='Path within the Persistent Disk that points to the raw image file',
      required=True)
  parser_googleclouddiskembedded.add_argument(
      '-d', '--disk_name', help='Google Cloud name for disk', required=True)
  parser_googleclouddiskembedded.add_argument(
      '-p', '--project', help='Project that the disk is associated with',
      required=True)
  parser_googleclouddiskembedded.add_argument(
      '-z', '--zone', help='Geographic zone the disk exists in',
      required=True)
  parser_googleclouddiskembedded.add_argument(
      '-s',
      '--source',
      help='Description of the source of the evidence',
      required=False)
  parser_googleclouddiskembedded.add_argument(
      '-n', '--name', help='Descriptive name of the evidence', required=False)

  # Parser options for Directory evidence type
  parser_directory = subparsers.add_parser(
      'directory', help='Process a directory as Evidence')
  parser_directory.add_argument(
      '-l', '--local_path', help='Local path to the evidence', required=True)
  parser_directory.add_argument(
      '-s',
      '--source',
      help='Description of the source of the evidence',
      required=False)
  parser_directory.add_argument(
      '-n', '--name', help='Descriptive name of the evidence', required=False)

  # List Jobs
  parser_listjobs = subparsers.add_parser(
      'listjobs', help='List all available jobs')

  # PSQ Worker
  parser_psqworker = subparsers.add_parser('psqworker', help='Run PSQ worker')
  parser_psqworker.add_argument(
      '-S',
      '--single_threaded',
      action='store_true',
      help='Run PSQ Worker in a single thread',
      required=False)

  # Parser options for Turbinia status command
  parser_status = subparsers.add_parser(
      'status',
      help='Get Turbinia Task status')
  parser_status.add_argument(
      '-a',
      '--all_fields',
      action='store_true',
      help='Show all task status fields in output',
      required=False)
  parser_status.add_argument(
      '-d',
      '--days_history',
      default=0,
      type=int,
      help='Number of days of history to show',
      required=False)
  parser_status.add_argument(
      '-r',
      '--request_id',
      help='Show tasks for given Request ID',
      required=False)
  parser_status.add_argument(
      '-t',
      '--task_id',
      help='Show task for given Task ID',
      required=False)
  parser_status.add_argument(
      '-w',
      '--wait',
      action='store_true',
      help='Wait until given task has completed execution to exit',
      required=False)

  # Server
  parser_server = subparsers.add_parser('server', help='Run Turbinia Server')

  args = parser.parse_args()
  if args.quiet:
    log.setLevel(logging.ERROR)
  elif args.verbose:
    log.setLevel(logging.INFO)
  elif args.debug:
    log.setLevel(logging.DEBUG)
  else:
    log.setLevel(logging.WARNING)

  # TaskManager
  # TODO(aarontp): Move some of this to turbinia/__init__.py
  config.LoadConfig()
  task_manager_ = task_manager.get_task_manager()
  task_manager_.setup()

  if args.output_dir:
    config.OUTPUT_DIR = args.output_dir
  if args.log_file:
    config.LOG_FILE = args.log_file

  evidence_ = None
  if args.command == 'rawdisk':
    args.name = args.name if args.name else args.local_path
    evidence_ = evidence.RawDisk(
        name=args.name, local_path=args.local_path, source=args.source)
  elif args.command == 'directory':
    args.name = args.name if args.name else args.local_path
    evidence_ = evidence.Directory(
        name=args.name, local_path=args.local_path, source=args.source)
  elif args.command == 'googleclouddisk':
    args.name = args.name if args.name else args.disk_name
    evidence_ = evidence.GoogleCloudDisk(
        name=args.name, disk_name=args.disk_name, project=args.project,
        zone=args.zone, source=args.source)
  elif args.command == 'googleclouddiskembedded':
    args.name = args.name if args.name else args.disk_name
    evidence_ = evidence.GoogleCloudDiskRawEmbedded(
        name=args.name, disk_name=args.disk_name,
        embedded_path=args.embedded_path, project=args.project, zone=args.zone,
        source=args.source)
  elif args.command == 'psqworker':
    # Set up root logger level which is normally set by the psqworker command
    # which we are bypassing.
    logger.setup(root=True)
    if args.single_threaded:
      worker = psq.Worker(queue=task_manager_.psq)
    else:
      worker = psq.MultiprocessWorker(queue=task_manager_.psq)
    log.info(
        'Starting PSQ listener on queue {0:s}'.format(task_manager_.psq.name))
    worker.listen()
  elif args.command == 'server':
    log.info('Running Turbinia Server.')
    task_manager_.run()
  elif args.command == 'status':
    # TODO(aarontp): This seems to be a valid assumption right now, but we
    # should figure out a better way to do this in case it changes.
    region = config.ZONE[:-2]
    PrintTaskStatus(
        instance=config.PUBSUB_TOPIC, project=config.PROJECT, region=region,
        days=args.days_history, task_id=args.task_id,
        request_id=args.request_id, all_fields=args.all_fields)
  elif args.command == 'listjobs':
    log.info('Available Jobs:')
    log.info(
        '\n'.join(['\t{0:s}'.format(job.name) for job in task_manager_.jobs]))
  else:
    log.warning('Command {0:s} not implemented.'.format(args.command))

  # If we have evidence to process and we also want to run as a server, then
  # we'll just process the evidence directly rather than send it through the
  # PubSub frontend interface.  If we're not running as a server then we will
  # create a new TurbiniaRequest and send it over PubSub.
  if evidence_ and args.server:
    task_manager_.add_evidence(evidence_)
    task_manager_.run()
  elif evidence_:
    request = TurbiniaRequest()
    request.evidence.append(evidence_)
    if args.dump_json:
      print request.to_json().encode('utf-8')
    else:
      log.info(
          'Creating PubSub request {0:s} with evidence {1:s}'.format(
              request.request_id, evidence_.name))
      task_manager_.server_pubsub.send_request(request)

  log.info('Done.')
  sys.exit(0)
